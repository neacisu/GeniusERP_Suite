# Docker Orchestration - GeniusSuite

## ğŸ“‹ Overview

Acest document descrie arhitectura Docker orchestration pentru GeniusSuite, implementatÄƒ conform strategiilor din documentaÈ›ia de arhitecturÄƒ.

## ğŸ—ï¸ ArhitecturÄƒ ReÈ›ele (Zero-Trust Model)

GeniusSuite foloseÈ™te 4 zone de reÈ›ea izolate conform Tabelul 3:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  net_edge (172.20.0.0/16)                                   â”‚

### AÈ™teptat: â‰¥20 Containere

- **4 Backing**: postgres, kafka, temporal, supertokens
- **4 Exportere**: postgres-metrics, kafka-metrics, temporal-metrics, neo4j-metrics
- **5 Observability**: prometheus, grafana, loki, promtail, otel-collector
- **7 CP Services**: identity, licensing, suite-admin, suite-shell, suite-login, ai-hub, analytics-hub

### Test Izolare & Export Metrici

```bash
# 1) Izolare â€“ rulatÄƒ din net_observability, trebuie sÄƒ eÈ™ueze cu "bad address"
docker run --rm --network geniuserp_net_observability busybox ping -c 1 postgres_server

# 2) Exportere â€“ rulÄƒrile de mai jos trebuie sÄƒ Ã®ntoarcÄƒ payload Prometheus
docker run --rm --network geniuserp_net_observability curlimages/curl:8.8.0 \
  curl -s http://postgres-metrics:9187/metrics | head -n 5
docker run --rm --network geniuserp_net_observability curlimages/curl:8.8.0 \
  curl -s http://kafka-metrics:9308/metrics | head -n 5
docker run --rm --network geniuserp_net_observability curlimages/curl:8.8.0 \
  curl -s http://temporal-metrics:8080/metrics | head -n 5
docker run --rm --network geniuserp_net_observability curlimages/curl:8.8.0 \
  curl -s http://neo4j-metrics:8080/metrics | head -n 5
```text

### Test Endpoints

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ net_backing_services     â”‚    â”‚  net_observability         â”‚
â”‚ (172.22.0.0/16)          â”‚    â”‚  (172.23.0.0/16)           â”‚
â”‚ - PostgreSQL             â”‚    â”‚  - Prometheus              â”‚
â”‚ - Kafka                  â”‚    â”‚  - Grafana                 â”‚
â”‚ - Temporal               â”‚    â”‚  - Loki                    â”‚
â”‚ - SuperTokens            â”‚    â”‚  - OTEL Collector          â”‚
â”‚ - Neo4j                  â”‚    â”‚  - Metrics sidecars        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Principii Zero-Trust

- **Backing services** (PostgreSQL, Kafka, etc.) nu sunt expuse pe net_edge È™i rÄƒmÃ¢n exclusiv pe net_backing_services
- **CP services** comunicÄƒ cu backing services DOAR prin net_backing_services
- **Observability** colecteazÄƒ metrici prin net_observability folosind sidecar/exporter-e dual-homed (ex. postgres-metrics)
- **Izolare completÄƒ** Ã®ntre zone

### Observability Sidecars

Pentru a respecta cerinÈ›a â€acces doar din net_backing_servicesâ€ È™i, simultan, pentru a expune metrici Ã®n net_observability, fiecare serviciu stateful are acum un **exporter** separat (ex. `postgres-metrics`, `temporal-metrics`). Exporter-ul se conecteazÄƒ la serviciul È›intÄƒ prin net_backing_services È™i expune `/metrics` doar Ã®n net_observability, eliminÃ¢nd necesitatea de a ataÈ™a containerele de date la reÈ›eaua de management.

## ğŸŒ Edge Proxy (Traefik)

- **FiÈ™ier compose:** `compose.yml` defineÈ™te serviciul Traefik È™i volumul persistent `gs_traefik_certs` montat la `/letsencrypt` pentru stocarea ACME (`acme.json`).
- **Config staticÄƒ/dinamicÄƒ:** `proxy/traefik/traefik.yml` stabileÈ™te entrypoints (80/443/8080/9100) È™i `proxy/traefik/dynamic/middlewares.yml` oferÄƒ middleware-uri (security headers, rate limit, basic-auth chain pentru dashboard).
- **FiÈ™ier env:** copiaÈ›i `proxy/.proxy.env.example` Ã®n `proxy/.proxy.env`, setaÈ›i `PROXY_DOMAIN`, `PROXY_DASHBOARD_DOMAIN`, `PROXY_DASHBOARD_USER/PASS`, email ACME È™i, opÈ›ional, token-urile DNS provider.
- **Pornire manualÄƒ:**

  ```bash
  set -a && source proxy/.proxy.env && set +a
  docker compose -f compose.yml up -d proxy
  ```
  
  Scriptul `scripts/start-suite.sh` ruleazÄƒ acest pas Ã®n FAZA 2, genereazÄƒ hash-ul BasicAuth (folosind `openssl passwd -apr1`) Ã®n `proxy/traefik/secrets/dashboard-users` È™i expune dashboard-ul doar pe `PROXY_DASHBOARD_DOMAIN` via entrypoint `traefik` (localhost:8080).
- **Observabilitate:** Traefik expune metrice Prometheus pe entrypoint `metrics` (9100) din `geniuserp_net_observability`, iar Prometheus le colecteazÄƒ prin job-ul `traefik`.

### Validare rapidÄƒ Traefik

```bash
# container up & sÄƒnÄƒtos
docker compose -f compose.yml ps proxy

# redirect HTTPâ†’HTTPS (foloseÈ™te porturile din PROXY_HTTP/HTTPS_PORT)
curl -I -H "Host: identity.${PROXY_DOMAIN}" http://127.0.0.1:${PROXY_HTTP_PORT}

# dashboard protejat (SNI + basic-auth)
curl -k -u "$PROXY_DASHBOARD_USER:$PROXY_DASHBOARD_PASS" \
  --resolve "${PROXY_DASHBOARD_DOMAIN}:${PROXY_DASHBOARD_PORT}:127.0.0.1" \
  https://${PROXY_DASHBOARD_DOMAIN}:${PROXY_DASHBOARD_PORT}/dashboard/ -o /dev/null -w '%{http_code}\n'

# Prometheus metrics (din interiorul reÈ›elei observability)
docker exec traefik wget -qO- http://localhost:9100/metrics | head -n 10

# verificÄƒ persistenÈ›a acme.json
docker exec traefik ls -l /letsencrypt
```

## ğŸš€ Pornire Infrastructure

### ComandÄƒ RapidÄƒ

```bash
cd /var/www/GeniusSuite
bash scripts/start-suite.sh
```

### Ordine de Pornire ManualÄƒ

#### 1. **Creare ReÈ›ele**

```bash
docker network create --driver bridge --subnet 172.20.0.0/16 geniuserp_net_edge
docker network create --driver bridge --subnet 172.21.0.0/16 geniuserp_net_suite_internal
docker network create --driver bridge --subnet 172.22.0.0/16 geniuserp_net_backing_services
docker network create --driver bridge --subnet 172.23.0.0/16 geniuserp_net_observability
```

#### 2. **Pornire Proxy (Traefik)**

```bash
cd /var/www/GeniusSuite
cp proxy/.proxy.env.example proxy/.proxy.env  # doar prima datÄƒ, apoi actualizeazÄƒ valorile reale
set -a && source proxy/.proxy.env && source shared/observability/.observability.env && set +a
docker compose -f compose.yml up -d proxy
```

> NotÄƒ: `gs_traefik_certs` pÄƒstreazÄƒ `acme.json`. Scriptul `scripts/start-suite.sh` regenereazÄƒ fiÈ™ierul BasicAuth Ã®n `proxy/traefik/secrets/dashboard-users` Ã®nainte de fiecare pornire.

#### 3. **Pornire Backing Services**

> Note: AsigurÄƒ-te cÄƒ existÄƒ fiÈ™ierele `.suite.general.env` È™i `.backing-services.env` (copiazÄƒ din variantele `.example` dacÄƒ rulezi prima datÄƒ).

```bash
cd /var/www/GeniusSuite
docker compose -f compose.yml up -d \
  postgres_server kafka temporal supertokens-core neo4j \
  postgres-metrics kafka-metrics temporal-metrics neo4j-metrics
```

VerificÄƒ healthy status:

```bash
docker ps --filter name=geniuserp --format 'table {{.Names}}\t{{.Status}}'
```

AÈ™teptat: 9 containere (postgres, kafka, temporal, supertokens, neo4j + exportere)

> **NotÄƒ Neo4j:** pentru ca endpoint-ul Prometheus sÄƒ fie acceptat de imagine, folosim `neo4j:5.23-enterprise` Ã®mpreunÄƒ cu `NEO4J_ACCEPT_LICENSE_AGREEMENT=yes`. Endpoint-ul intern ruleazÄƒ pe `0.0.0.0:2004`, iar sidecar-ul `neo4j-metrics` Ã®l proxy-uieÈ™te spre `net_observability` pe portul 8080.

#### 4. **Pornire Observability Stack**

```bash
cd /var/www/GeniusSuite
set -a && source shared/observability/.observability.env && set +a
docker compose -f compose.yml up -d otel-collector tempo prometheus grafana loki promtail
```

Accesare UI:

- **Grafana**: `http://localhost:3000 (admin/admin)`
- **Prometheus**: `http://localhost:9090`
- **Temporal UI**: disponibil doar Ã®n `geniuserp_net_backing_services` (nu este expus pe host) â€” metricile sunt expuse via `temporal-metrics`

#### 5. **Pornire CP Services**

âš ï¸ **IMPORTANT**: Environment variables trebuie Ã®ncÄƒrcate Ã®nainte de build/start:

```bash
cd /var/www/GeniusSuite

# Identity
set -a && source .suite.general.env && source cp/identity/.cp.identity.env && set +a
docker compose -f cp/identity/compose/docker-compose.yml up -d

# Licensing
set -a && source .suite.general.env && source cp/licensing/.cp.licensing.env && set +a
docker compose -f cp/licensing/compose/docker-compose.yml up -d

# AI Hub
set -a && source .suite.general.env && source cp/ai-hub/.cp.ai-hub.env && set +a
docker compose -f cp/ai-hub/compose/docker-compose.yml up -d

# Analytics Hub
set -a && source .suite.general.env && source cp/analytics-hub/.cp.analytics-hub.env && set +a
docker compose -f cp/analytics-hub/compose/docker-compose.yml up -d
```

#### PregÄƒtire ReÈ›ele & Volume (F0.4.19)

```bash
cd /var/www/GeniusSuite
bash scripts/compose/init-infra.sh
```

Scriptul creeazÄƒ toate reÈ›elele externe (`geniuserp_net_*`) È™i volumele marcate `external: true` Ã®nainte de a porni orice compose local. Rularea este idempotentÄƒ È™i poate fi repetatÄƒ dupÄƒ un clean host sau pe CI.

#### Validare CP Hybrid (Identity, Suite Shell & Suite Admin)

```bash
# Identity (F0.4.5)
set -a && source .suite.general.env && source cp/identity/.cp.identity.env && set +a && \
  docker compose -f cp/identity/compose/docker-compose.yml up -d
docker ps --filter name=genius-suite-identity --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'
docker exec genius-suite-identity nc -zv postgres_server 5432
docker exec genius-suite-identity wget -qO- http://supertokens-core:3567/hello
docker exec traefik wget -qO- http://identity:6250/health

# Suite Shell (F0.4.6)
set -a && source .suite.general.env && source cp/suite-shell/.cp.suite-shell.env && set +a && \
  docker compose -f cp/suite-shell/compose/docker-compose.yml up -d
docker ps --filter name=genius-suite-shell --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'
docker exec genius-suite-shell wget -qO- http://identity:6250/health
curl -I http://localhost:6100/health
docker exec traefik wget -qO- http://suite-shell:6100/health

# Suite Admin (F0.4.7)
set -a && source .suite.general.env && source cp/suite-admin/.cp.suite-admin.env && set +a && \
  docker compose -f cp/suite-admin/compose/docker-compose.yml up -d
docker ps --filter name=genius-suite-admin --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'
docker exec genius-suite-admin nc -zv postgres_server 5432
docker exec genius-suite-admin wget -qO- http://identity:6250/health
curl -I http://localhost:6150/health
docker exec traefik wget -qO- http://suite-admin:6150/health
```

## ğŸ›‘ Oprire Infrastructure

### 1. ComandÄƒ RapidÄƒ

```bash
cd /var/www/GeniusSuite
bash scripts/stop-suite.sh
```

### 2. Oprire ManualÄƒ (Ordine InversÄƒ)

```bash
# CP Services
docker compose -f cp/analytics-hub/compose/docker-compose.yml down
docker compose -f cp/ai-hub/compose/docker-compose.yml down
docker compose -f cp/licensing/compose/docker-compose.yml down
docker compose -f cp/identity/compose/docker-compose.yml down

# Observability
cd /var/www/GeniusSuite
set -a && source shared/observability/.observability.env && set +a
docker compose -f compose.yml stop otel-collector tempo prometheus grafana loki promtail
docker compose -f compose.yml rm -f otel-collector tempo prometheus grafana loki promtail

# Backing Services
cd /var/www/GeniusSuite
docker compose -f compose.yml stop postgres_server kafka temporal supertokens-core neo4j \
  postgres-metrics kafka-metrics temporal-metrics neo4j-metrics
docker compose -f compose.yml rm -f postgres_server kafka temporal supertokens-core neo4j \
  postgres-metrics kafka-metrics temporal-metrics neo4j-metrics
```

âš ï¸ **NU folosiÈ›i `-v` flag** - volumele sunt externe È™i trebuie pÄƒstrate!

## ğŸ”„ Rebuild Serviciu (FÄƒrÄƒ Pierdere Date)

### Exemplu: Rebuild Identity Service

```bash
cd /var/www/GeniusSuite

# 1. Stop containerul
docker compose -f cp/identity/compose/docker-compose.yml down

# 2. Build nou (cu env variables)
set -a && source .suite.general.env && source cp/identity/.cp.identity.env && set +a
docker compose -f cp/identity/compose/docker-compose.yml build

# 3. Start cu noua imagine
docker compose -f cp/identity/compose/docker-compose.yml up -d

# 4. VerificÄƒ logs
docker logs genius-suite-identity --tail 50
```

### ProtecÈ›ie Date

Datele persistÄƒ datoritÄƒ **volumelor externe**:

- `gs_pgdata_*` - Baze de date PostgreSQL
- `gs_kafka_data` - Kafka topics
- `geniuserp_loki_data` - Loki logs

Acestea sunt definite cu `external: true` Ã®n compose files, deci nu se È™terg la `docker compose down`.

## ğŸ¥ Health Checks

### Verificare Status Complet

```bash
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'
```

### AÈ™teptat: 16 Containere

- **4 Backing**: postgres, kafka, temporal, supertokens
- **5 Observability**: prometheus, grafana, loki, promtail, otel-collector
- **7 CP Services**: identity, licensing, suite-admin, suite-shell, suite-login, ai-hub, analytics-hub

### Test Endpoints

```bash
# PostgreSQL
docker exec geniuserp-postgres pg_isready -U suite_admin

# Identity API
curl http://localhost:6250/health

# Licensing API
curl http://localhost:6300/health

# AI Hub
curl http://localhost:6400/health

# Analytics Hub
curl http://localhost:6350/health

# Grafana
curl http://localhost:3000/api/health
```

## ğŸ”§ Troubleshooting

### 1. Container "unhealthy"

```bash
# VerificÄƒ health check logs
docker inspect <container_name> --format '{{json .State.Health}}' | jq

# VerificÄƒ logs aplicaÈ›ie
docker logs <container_name> --tail 100
```

**Cauze frecvente:**

- Health check foloseÈ™te `localhost` Ã®n loc de `127.0.0.1` (alpine DNS issue)
- Port greÈ™it Ã®n health check test
- AplicaÈ›ia nu e ready Ã®n timpul `start_period`

### 2. "invalid proto:" Error

CauzÄƒ: ReferinÈ›e `depends_on` cÄƒtre servicii din alte compose files.

**Fix**: Eliminat toate `depends_on` pentru servicii externe. Orchestrarea se face manual prin scripts/start-suite.sh.

### 3. PostgreSQL "POSTGRES_PASSWORD not specified"

CauzÄƒ: Variable substitution `${VAR}` nu funcÈ›ioneazÄƒ Ã®n CONNECTION_URI.

**Fix**: Folosit parametri separaÈ›i:

```yaml
POSTGRESQL_HOST: postgres_server
POSTGRESQL_PORT: 5432
POSTGRESQL_USER: suite_admin
POSTGRESQL_PASSWORD: ${SUITE_DB_POSTGRES_PASS:-ChangeThisPostgresPassword}
POSTGRESQL_DATABASE_NAME: identity_db
```

### 4. Kafka "unhealthy"

CauzÄƒ: Health check script nu e Ã®n PATH.

**Fix**: Folosit full path:

```yaml
healthcheck:
  test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
```

### 5. SuperTokens connection error

CauzÄƒ: Baza de date `identity_db` nu existÄƒ (PostgreSQL nu creeazÄƒ automat bazele multiple).

**Fix**: Create manual:

```bash
docker exec geniuserp-postgres psql -U suite_admin -d postgres -c "CREATE DATABASE identity_db;"
```

### 6. Environment variables nu se Ã®ncarcÄƒ

CauzÄƒ: Docker Compose nu Ã®ncarcÄƒ automat .env files la build.

**Fix**: Source explicit Ã®nainte de comenzi:

```bash
set -a && source .suite.general.env && source cp/service/.cp.service.env && set +a
docker compose -f cp/service/compose/docker-compose.yml build
```

### 7. OTEL Collector connection refused

CauzÄƒ: OTEL Ã®ncearcÄƒ sÄƒ se conecteze la Tempo (care nu existÄƒ).

**Status**: Non-blocker - serviciile funcÈ›ioneazÄƒ fÄƒrÄƒ tracing complet.

## ğŸ“Š Porturi Alocate (Tabelul 4 & 5)

### Backing Services

| Service | Port | Protocol |
|---------|------|----------|
| PostgreSQL | 5432 | TCP |
| Kafka | 9092 | TCP |
| Temporal | 7233 | gRPC |
| Temporal UI | 8233 | HTTP |
| SuperTokens | 3567 | HTTP |

### Observability

| Service | Port | Protocol |
|---------|------|----------|
| Prometheus | 9090 | HTTP |
| Grafana | 3000 | HTTP |
| Loki | 3100 | HTTP |
| OTEL gRPC | 4317 | gRPC |
| OTEL HTTP | 4318 | HTTP |

### Control Plane Services

| Service | Port | Range | Status |
|---------|------|-------|--------|
| Suite Shell | 6100 | 6100-6149 | âœ… Operational |
| Suite Admin | 6150 | 6150-6199 | âœ… Operational |
| Suite Login | 6200 | 6200-6249 | âœ… Operational |
| Identity | 6250 | 6250-6299 | âœ… Operational |
| Licensing | 6300 | 6300-6349 | âœ… Operational |
| Analytics Hub | 6350 | 6350-6399 | âœ… Operational |
| AI Hub | 6400 | 6400-6449 | âœ… Operational |

## ğŸ“š ReferinÈ›e Strategii

- **Tabelul 2.4**: Volume management strategy
- **Tabelul 3**: Network architecture (4 zones)
- **Tabelul 3.5**: Service-to-network mapping
- **Tabelul 4**: Infrastructure ports allocation
- **Tabelul 5**: Application ports allocation
- **SecÈ›iunea 2.2**: Data protection strategy

## âš ï¸ Troubleshooting Archive

### Historical Issues (RESOLVED âœ…)

#### Historical Issue 1: Container "unhealthy" - Health check alpine DNS

- **CauzÄƒ**: Health check foloseÈ™te `localhost` Ã®n loc de `127.0.0.1` (alpine DNS issue)
- **Fix**: âœ… Toate health checks actualizate sÄƒ foloseascÄƒ `127.0.0.1` È™i CMD-SHELL format

```yaml
healthcheck:
  test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:6250/health || exit 1"]
```

#### Historical Issue 2: "invalid proto:" error

- **CauzÄƒ**: ReferinÈ›e `depends_on` cÄƒtre servicii din alte compose files
- **Fix**: âœ… Eliminat toate `depends_on` pentru servicii externe; orchestrarea se face manual prin `scripts/start-suite.sh`

#### Historical Issue 3: PostgreSQL "POSTGRES_PASSWORD not specified"

- **CauzÄƒ**: Variable substitution `${VAR}` nu funcÈ›ioneazÄƒ Ã®n CONNECTION_URI
- **Fix**: âœ… Folosit parametri separaÈ›i

```yaml
POSTGRESQL_HOST: postgres_server
POSTGRESQL_PORT: 5432
POSTGRESQL_USER: suite_admin
POSTGRESQL_PASSWORD: ${SUITE_DB_POSTGRES_PASS:-ChangeThisPostgresPassword}
POSTGRESQL_DATABASE_NAME: identity_db
```

#### Historical Issue 4: Kafka "unhealthy"

- **CauzÄƒ**: Health check script nu e Ã®n PATH
- **Fix**: âœ… Folosit full path

```yaml
healthcheck:
  test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
```

#### Historical Issue 5: SuperTokens connection error

- **CauzÄƒ**: Baza de date `identity_db` nu existÄƒ (PostgreSQL nu creeazÄƒ automat bazele multiple)
- **Fix**: âœ… Creat manual

```bash
docker exec geniuserp-postgres psql -U suite_admin -d postgres -c "CREATE DATABASE identity_db;"
```

#### Historical Issue 6: Dockerfile pnpm installation failures (suite-admin/shell/login)

- **CauzÄƒ**: Scriptul `wget` pentru instalarea pnpm a eÈ™uat Ã®n containere alpine
- **Fix**: âœ… Ãnlocuit cu `npm install -g pnpm`

```dockerfile
RUN npm install -g pnpm
ENV PATH="/usr/local/bin:$PATH"
RUN pnpm install --frozen-lockfile
```

#### Historical Issue 7: OTEL Collector connection refused + Tempo errors

- **CauzÄƒ**: OTEL Ã®ncerca sÄƒ se conecteze la Tempo (inexistent) È™i folosea exporter-ul `logging` depreciat
- **Fix**: âœ… Tempo exporter a fost comentat, `logging` a fost Ã®nlocuit cu `debug`

```yaml
exporters:
  debug:
    verbosity: detailed
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    traces:
      exporters: [debug]
    metrics:
      exporters: [prometheus, debug]
```

Result: OTEL ascultÄƒ pe 4317/4318, iar CP services se conecteazÄƒ cu succes.

#### Historical Issue 8: Temporal gRPC connection warnings

- **CauzÄƒ**: Temporal asculta doar pe observability network IP, nu pe `0.0.0.0`
- **Fix**: âœ… AdÄƒugat variabila de mediu `BIND_ON_IP=0.0.0.0`

```yaml
environment:
  - BIND_ON_IP=0.0.0.0
```

Result: Licensing â†’ Temporal gRPC connection successful (test cu `nc -zv temporal 7233`).

## ğŸ¯ Current Status

âœ… **ALL SYSTEMS OPERATIONAL** - 16/16 containere funcÈ›ionale

**Infrastructure Complete:**

- âœ… 4 Backing Services (postgres, kafka, temporal, supertokens) - toate healthy
- âœ… 5 Observability Services (prometheus, grafana, loki, promtail, otel-collector) - toate funcÈ›ionale
- âœ… 7 Control Plane Services - toate healthy pe porturile alocate

**Connectivity Verified:**

- âœ… CP â†’ PostgreSQL
- âœ… CP â†’ Kafka
- âœ… CP â†’ OTEL Collector (4317 gRPC)
- âœ… Licensing â†’ Temporal (7233 gRPC)
- âœ… Zero-Trust architecture (net_edge izolat)

**Data Persistence Verified:**

- âœ… PostgreSQL volumes persist through container rebuild
- âœ… External volumes strategy funcÈ›ioneazÄƒ conform Tabelul 2.4

## ğŸ¯ Next Steps

1. ~~Fix Dockerfiles pentru suite-admin/shell/login~~ âœ… COMPLETE
2. ~~Configure OTEL Collector sÄƒ accepte traces fÄƒrÄƒ Tempo backend~~ âœ… COMPLETE  
3. ~~Fix Temporal gRPC binding~~ âœ… COMPLETE
4. Add application-level metrics collection Ã®n toate CP services
5. Implement graceful shutdown Ã®n stop-suite.sh (wait for drain)
6. Add backup/restore scripts pentru PostgreSQL volumes
7. Deploy Tempo backend pentru distributed tracing (optional)

---

**Versiune**: 2.0  
**Data**: 2025-11-13  
**Status**: âœ… ALL SYSTEMS OPERATIONAL - 16/16 containere
